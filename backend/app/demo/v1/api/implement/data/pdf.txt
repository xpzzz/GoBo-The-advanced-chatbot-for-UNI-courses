Entity-Relationship (ER) Diagrams:ER diagrams are a graphical tool for data modelling.   
Entity Sets:An entity set can be viewed as either: 1.   a set of entities with the same set of attributes (extensional view of entity set) 2.   an abstract description of a class of entities (intentional view of entity set)
Key (superkey):any set of attributes whose set of values are distinct over entity set 
candidate key:any superkey such that no proper subset of its attributes is also a superkey.  
primary key:one candidate key chosen by the database designer 
Relationship Sets:collection of relationships of the same type.  
Relationship Types:1.   one to one 2.   one to many 3.   many to many
Weak Entity Sets:1.   exist only because of association with strong entities 2.   have no key of their own; have a discriminator.  
Properties of subclasses:1.   overlapping or disjoint 2.   total or partial
Integrity Constraints:Relations are used to represent entities and relationships.   Domains limit the set of values that attributes can take.   To represent real-world problems, need to describe: 1.   what values are/are not allowed 2.   what combinations of values are/are not allowed
Referential Integrity:1.  describe references between relations (tables) 2.  are related to notion of a foreign key (FK).  
Relational Databases:ensure that all data in the database satisfies constraints.  
ER to Relational Mapping:Differences between relational and ER models: 1.  Rel uses relations to model entities and relationships 2.  Rel has no composite or multi-valued attributes (only atomic) 3.  Rel has no object-oriented notions (e.  g.   subclasses, inheritance)
Mapping Strong Entities:An entity set E with atomic attributes a 1 , a 2 , .  .  .   a maps to A relation R with attributes (columns) a 1 , a 2 , .  .  .   an.  
RDBMS:A relational database management system, which is: software designed to support large-scale data-intensive applications allowing high-level description of data (tables, constraints) with high-level access to the data (relational model, SQL) providing efficient storage and retrieval (disk/memory management) supporting multiple simultaneous users (privilege, protection) doing multiple simultaneous operations (transactions, concurrency) maintaining reliable access to the stored data (backup, recovery)
RDBMS Operations:create/remove a database or a schema create/remove/alter tables within a schema insert/delete/update tuples within a table queries on data, define named queries (views) transactional behaviour (ACID)
Managing Databases:Shell commands: 1.  createdb dbname 2.  dropdb dbname (If no dbname supplied, assumes a database called YOU) SQL statements: 1.  CREATE DATABASE dbname 2.  DROP DATABASE dbname
SQL:SQL has several sub-languages:1.  meta-data definition language (e.  g.   CREATE TABLE) 2.  meta-data update language (e.  g.   ALTER TABLE) 3.  data update language (e.  g.   INSERT, UPDATE, DELETE) query language (SQL) (e.  g.   SELECT) Meta-data languages manage the schema.   Data languages manipulate (sets of) tuples.   Query languages are based on relational algebra.  
SQL Syntax:SQL definitions, queries and statements are composed of: comments, identifiers, keywords, data types, operators, and constants.  
Types/Constants in SQL:1.  Numeric types: INTEGER, REAL, NUMERIC 2.  String types: CHAR(n), VARCHAR(n) 3.  Logical type: BOOLEAN, TRUE and FALSE 4.  Subtraction of timestamps yields an interval
Tuple and Set Literals:Tuple and set constants are both written as:( val 1 , val 2 , val 3 , .  .  .   ) The correct interpretation is worked out from the context.  
SQL Operators:Comparison operators are defined on all types: < > <= >= = <>
The NULL Value:Expressions containing NULL generally yield NULL.   However, boolean expressions use three-valued logic.  
Conditional Expressions:Other ways that SQL provides for dealing with NULL
SQL Data Definition:Defines table schema and creates empty instance of table.  
Primary Keys:If we want to define a numeric primary key, e.  g.   CREATE TABLE R ( id INTEGER PRIMARY KEY, .  .  .   ); we still have the problem of generating unique values.   Most DBMSs provide a mechanism to generate a sequence of unique values.   ensure that two tuples don't get assigned the same value
Referential Integrity:Declaring foreign keys assures referential integrity.  
SQL Query Language:SQL provides powerful, high-level manipulation of data.   However, SQL is not a complete programming language.  
SQL Query Language:An SQL query consists of a sequence of clauses: SELECT projectionList FROM relations/joins WHERE condition GROUP BY groupingAttributes HAVING groupCondition
Problem-solving in SQL:Request: description of required information from database.   Pre-req: know your schema
Views:A view associates a name with a query
SQL Problem-solving:Steps in solving problems in SQL: know the schema, read the query request identify components of result tuples identify relevant data items and tables in schema build intermediate result tables (joins) combine intermediate tables to produce result compute values to appear in result tuples
Stored Procedures:functions that are stored in DB along with data written in a language combining SQL and procedural ideas provide a way to extend operations available in database executed within the DBMS (close coupling with query engine)
Function Return Types:A PostgreSQL function can return a value which is: void (i.  e.   no return value) an atomic data type (e.  g.   integer, text, .  .  .  ) a tuple (e.  g.   table record type or tuple type) a set of atomic values (like a table column) a set of tuples (i.  e.   a table)   A function returning a set of values is similar to a view.  
SQL Functions:PostgreSQL allows functions to be defined in SQL
PLpgSQL:PLpgSQL = Procedural Language extensions to PostgreSQL
Limitations of Basic SQL:This is not sufficient to write complete applications.   Need to write programs to manipulate database.   Extending database functionality would also help.  
Extending SQL:Ways in which standard SQL might be extended: new data types (incl.   constraints, I/O, indexes, .  .  .  ) more operations/aggregates for use in queries more powerful constraint checking event-based triggered actions different kinds of queries (e.  g.   recursive) All are potentially useful in application development.  
New Data Types:SQL data definition language provides: 1.  atomic types: integer, float, character, boolean 2.  ability to define tuple types (create table)
New Functions:SQL provides for new functions via stored procedures.   PostgreSQL has functions in: SQL, PLpgSQL, Python.  .  .  
Advanced Query Types:Many specialised types of query have been identified.   We have seen: select/project/join, aggregation, grouping Many modern queries (e.  g.   skyline) come from OLAP.  
Window Functions:Group-by allows us to: 1.  summarize a set of tuples 2.  that have common values for a set of attributes
Aggregates:Aggregates reduce a collection of values into a single result.  
User-defined Aggregates:SQL standard does not specify user-defined aggregates.   But PostgreSQL provides a mechanism for defining them.  
Constraints:Column and table constraints ensure validity of one table.   RI constraints ensure connections between tables are valid.   However, specifying validity of entire database often requires constraints involving multiple tables.  
Assertions:Assertions are schema-level constraints: 1.  typically involving multiple tables 2.  expressing a condition that must hold at all times 3.  need to be checked on each change to relevant tables 4.  if change would cause check to fail, reject change
Triggers:Triggers are: 1.  procedures stored in the database 2.  activated in response to database events
Trigger Semantics:Triggers can be activated BEFORE or AFTER the event.   If activated BEFORE, can affect the change that occurs: 1.  NEW contains 'proposed' value of changed tuple 2.  modifying NEW causes a different value to be placed in DB.   If activated AFTER, the effects of the event are visible: 1.  NEW contains the current value of the changed tuple 2.  OLD contains the previous value of the changed tuple 3.  constraint-checking has been done for NEW
Triggers in PostgreSQL:PostgreSQL triggers provide amechanism for: 1.  INSERT, DELETE or UPDATE events 2.  to automatically activate PLpgSQL functions
Triggers:Triggers are actions invoked by DB modifications.   They allow programmers to implement global constraint (assertion) checking maintain summary values (cross-table dependencies) They achieve this by invoking functions before/after insert/delete/update using/manipulating OLD/NEW values of changed tuples
Programming with Databases:Complete applications require code outside the DBMS  to handle the user interface (GUI or Web) to interact with other systems (e.  g.   other DBs) to perform compute-intensive work (vs.   data-intensive) 'Conventional' programming languages (PLs) provide these.  
PL/DB Interface:Common DB access API used in programming languages.   This pattern is used in many different libraries: Java/JDBC, PHP/PDO, Perl/DBI, Python/dbapi2, Tcl.  .  .  
DB Library:Functions in the database library: dbConnect(conn): establish connection to DB dbQuery(db,sql): send SQL statement for execution dbNext(res): fetch next tuple from result set dbUpdate(db,sql): send SQL insert/delete/update
DB/PL Mismatch:There is a tension between PLs and DBMSs DBMSs deal efficiently with sets of tuples PLs encourage dealing with single tuples/objects If not handled carefully, can lead to inefficient use of DB.   Note: relative costs of DB access operations: establishing a DBMS connection .  .  .   very high initiating an SQL query .  .  .   high accessing individual tuple .  .  .   low
PL access to DBs:In accessing DBs from PLs, your code contains: code in a programming language SQL query/update statements code to map between tuples and PL data When you write PHP/DB scripts for web pages: also get HTML, JavaScript, Cascading Style Sheet (CSS)
Catalogs:An RDBMS maintains a collection of relation instances.   To do this, it also needs information about relations: name, owner, primary key of each relation name, data type, constraints for each attribute authorisation for operations on each relation Similarly for other DBMS objects (e.  g.   views, functions, triggers.  .  .  ) This information is stored in the system catalog.  
PostgreSQL Catalog:Most DBMSs had defined their own catalog tables before INFORMATION_SCHEMA was standardised.   The PostgreSQL catalog contains around 80 tables and views most describe schema data (tables, attributes, constraints, .  .  .  ) others deal with DB configuration and statistics others deal with users, roles, privileges all are called pg_XXX (e.  g.   pg_tables) many have primary key via implicit oid attribute This week's lecture 'PostgreSQL Catalog' contains details of important catalog tables.   For full details, see Chapter 50 (Systems Catalog) in PostgreSQL documentation.  
Database Access Control:Access to DBMSs involves two aspects: having execute permission for a DBMS client (e.  g.   psql) having a username/password registered in the DBMS Establishing a connection to the database: user supplies database/username/password to client client passes these to server, which validates them if valid, user is 'logged in' to the specified database
Database Access Control in PostgreSQL:PostgreSQL has two ways to create users.   From the Unix command line, via the command.   From SQL, via the statement.  
SQL Access Control:SQL access control deals with privileges on database objects (e.  g.   tables, view, functions, .  .  .  ) allocating such privileges to roles (i.  e.   users and groups) The user who creates an object is automatically assigned:  ownership of that object a privilege to modify (ALTER) the object a privilege to remove (DROP) the object along with all other privileges specified below
Relational Design Theory:The study of relational design theory examines some foundational notions of 'schema goodness', and provides methods to transform schemas to make them better
Functional dependencies:are constraints between attributes within a relation,and have implications for 'good' relational schema design
good relational database design:must capture all necessary attributes/associations, and do this with a minimal amount of stored information
Notation/Terminology:Most texts adopt the following terminology: Relation schemas Relation instances Tuples Attributes Sets of attributes Attributes in tuples upper-case letters, denoting set of all attributes (e.  g.   R, S, P, Q ) lower-case letter corresponding to schema (e.  g.   r(R), s(S), p(P), q(Q) ) lower-case letters (e.  g.   t, t', t 1 , u, v ) upper-case letters from start of alphabet (e.  g.   A, B, C, D ) simple concatenation of attribute names (e.  g.   X=ABCD, Y=EFG ) tuple[attrSet] (eg, t[ABCD], t[X])
Functional Dependency:Much more important for design is the notion of dependency across all possible instances of the relation (i.  e.   a schema-based dependency).  
Inference Rules:Armstrong's rules are complete, general rules of inference on fds.  
Closures:Given a set F of fds, how many new fds can we derive? For a finite set of attributes, there must be a finite set of fds.   The largest collection of dependencies that can be derived from F is called the closure of F and is denoted F + .   Closures allow us to answer two interesting questions: 1.  is a particular dependency X → Y derivable from F? 2.  are two sets of dependencies F and G equivalent?
Normalization:branch of relational theory providing design insights.   The goals of normalization:1.  be able to characterise the level of redundancy in a relational schema 2.  provide mechanisms for transforming schemas to remove redundancy
Normal Forms:Normalization theory defines six normal forms (NFs).   1.  First,Second,Third Normal Forms (1NF,2NF,3NF) (Codd 1972) 2.  Boyce-Codd Normal Form (BCNF) (1974) 3.  Fourth Normal Form (4NF) (Zaniolo 1976, Fagin 1977) 4.  Fifth Normal Form (5NF) (Fagin 1979) NF hierarachy: 5NF ⇒ 4NF ⇒ BCNF ⇒ 3NF ⇒ 2NF ⇒ 1NF 1NF allows most redundancy; 5NF allows least redundancy.  
Relation Decomposition:The standard transformation technique to remove redundancy:   decompose relation R into relations S and T We accomplish decomposition by: 1.  selecting (overlapping) subsets of attributes 2.  forming new relations based on attribute subsets
Boyce-Codd Normal Form:If we transform a schema into BCNF, we are guaranteed: 1.  no update anomalies due to fd-based redundancy 2.  lossless join decomposition.   However, we are not guaranteed: the new schema preserves all fds from the original schema.   This may be a problem if the fds contain significant semantic information about the problem domain.   If we need to preserve dependencies, use 3NF.  
Third Normal Form:If we transform a schema into 3NF, we are guaranteed: 1.  lossless join decomposition 2.  the new schema preserves all of the fds from the original schema
Database Design Methodology:To achieve a 'good' database design:   1.  identify attributes, entities, relationships → ER design 2.  map ER design to relational schema 3.  identify constraints (including keys and functional dependencies) 4.  apply BCNF/3NF algorithms to produce normalized schema
Database Application Performance:In order to make DB applications efficient, we need to know:   1.  what operations on the data does the application require (which queries, updates, inserts and how frequently is each one performed) 2.  how these operations might be implemented in the DBMS (data structures and algorithms for select, project, join, sort, .  .  .  ) 3.  how much each implementation will cost (in terms of the amount of data transferred between memory and disk)
DBMS Components:File manager, Buffer manager, Query optimiser, Recovery manager, Concurrency manager, Integrity manager
Relational Algebra:Relational algebra (RA) can be viewed as: 1.  mathematical system for manipulating relations, or 2.  data manipulation language (DML) for the relational model Relational algebra consists of: 1.  operands: relations, or variables representing relations 2.  operators that map relations to relations 3.  rules for combining operands/operators into expressions 4.  rules for evaluating such expressions
Notation:Standard treatments of relational algebra use Greek symbols.  
Rename:Rename provides 'schema mapping'
Selection:Selection returns a subset of the tuples in a relation r(R) that satisfy a specified condition C.  
Projection:Projection returns a set of tuples containing a subset of the attributes in the original relation.  
Union:Union combines two compatible relations into a single relation via set union of sets of tuples.  
Intersection:Intersection combines two compatible relations into a single relation via set intersection of sets of tuples.  
Difference:Difference finds the set of tuples that exist in one relation but do not occur in a second compatible relation.  
Product:Product (Cartesian product) combines information from two relations pairwise on tuples.  
Natural Join:Natural join is a specialised product: 1.  containing only pairs that match on common attributes 2.  with one of each pair of common attributes eliminated
Theta Join:The theta join is a specialised product containing only pairs that match on a supplied condition C.  
Division:Consider two relation schemas R and S where S ⊂ R.  
Query Processing:Goal of query processing: 1.  find/build a set of tuples satisfying some condition 2.  tuples may be constructed using values from multiple tables
Query Optimisation:The query optimiser start with an RA expression, then: 1.  generates a set of equivalent expressions 2.  generates possible execution plans for each 3.  estimates cost of each plan, chooses chepaest
Transaction processing:techniques for managing 'logical units of work' which may require multiple DB operations
Concurrency control:techniques for ensuring that multiple concurrent transactions do not interfere with each other
Recovery mechanisms:techniques to restore information to a consistent state, even after major hardware shutdowns/failures
Transactions:1.  an atomic 'unit of work' in an application 2.  which may require multiple database changes
Transaction Concepts:A transaction must always terminate, either successfully (COMMIT), with all changes preserved or unsuccessfully (ABORT), with database unchanged
Transaction Consistency:Transactions typically have intermediate states that are inconsistent.   However, states before and after transaction must be consistent.  
Serial Schedules:Serial execution: T1 then T2 or T2 then T1
Concurrent Schedules:Concurrent schedules interleave T1,T2,,, operations
Serializability:lizable schedule: 1.  concurrent schedule for T 1 .  .  T n with final state S 2.  S is also a final state of one of the possible 3.  serial schedules for T 1 .  .  T n
Concurrency Control:Serializability tests are useful theoretically.   But don't provide a mechanism for organising schedules 1.  they can only be done 'after the event' 2.  they are computationally very expensive O(n!)What is required are methods that: 1.  can be applied to each transaction individually 2.  guarantee that overall schedule is serializable
Lock-based Concurrency Control:Synchronise access to shared data items via following rules: 1.  before reading X, get shared (read) lock on X 2.  before writing X, get exclusive (write) lock on X 3.  an attempt to get a shared lock on X is blocked if another transaction already has exclusive lock on X 4.  an attempt to get an exclusive lock on X is blocked if another transaction has any kind of lock on X
Multi-version Concurrency Control:One approach to reducing the requirement for locks is to: 1.  provide multiple (consistent) versions of the database 2.  give each transaction access to an 'appropriate' version
Future of Database:Extend the relational model: 1.  add new data types and query ops for new applications 2.  deal with uncertainty/inaccuracy/approximation in data Replace the relational model: 1.  object-oriented DBMS  OO programming with persistent objects 2.  XML DBMS .  .  .   all data stored as XML documents, new query model 3.  application-effective data model (e.  g.   (key,value) pairs)
Big Data:Some modern applications have massive data sets (e.  g.   Google) which is far too large to store on a single machine/RDBMS and the query demands far too high even if could store in DBMS
Information Retrieval:DBMSs generally do precise matching (although like/regexps) Information retrieval systems do approximate matching.  
Multimedia Data:Data which does not fit the 'tabular model': image, video, music, text, .  .  .   (and combinations of these).   Research problems: 1.  how to specify queries on such data? (image 1 ≅ image 2 ) 2.  how to 'display' results? (synchronize components) Solutions to the first problem typically: 1.  extend notions of 'matching'/indexes for querying 2.  require sophisticated methods for capturing data features
Stream Management Systems:Makes one addition to the relational model: stream = infinite sequence of tuples, arriving one-at-a-time
Semi-structured Data:Uses graphs rather than tables as basic data structure tool
Dispersed Databases:1.  very large numbers of small processing nodes 2.  data is distributed/shared among nodes
